# limefortime
## Explainable AI for Signal Classification

Intrauterine Growth Restriction (IUGR) is a condition where the fetus fails to grow at the expected rate during pregnancy, posing significant risks to perinatal health. 

Cardiotocography (CTG) is one of the most widely used technique for assessing fetal wellbeing by monitoring Fetal Heart Rate (FHR) and uterine contractions (TOCO) signals during pregnancy.  This technique provides valuable information on fetal health and can identify potential problems such as fetal distress and intrauterine growth restriction (IUGR).

IUGR can cause stillbirth, premature birth, and even developmental issues in the brain of the fetus. In many cases are related to reduced nutrient and oxygen supply. Current surveillance methods like fetal Doppler ultrasound and heart rate monitoring fail to detect nearly 50% of IUGR cases. A significant number of IUGR-related stillbirths occur without prior diagnosis of IUGR. [1] These challenges align well with the capabilities of machine learning (ML) and deep learning (DL) models, which can provide more accurate and early detection of IUGR.

ML/DL models can analyze large datasets to identify complex, multidimensional patterns and correlations that characterize pathological conditions which are not easily detectable by conventional methods. By training on comprehensive datasets, these models can improve the sensitivity and specificity of IUGR detection. These models also can analyze longitudinal data to detect changes over time, providing a dynamic and continuous assessment of fetal growth and health. This approach can identify early signs of IUGR by monitoring trends and deviations from expected growth patterns. However, medical practitioners require transparent and interpretable models to trust and effectively use the deep learning model predictions. Explainable AI (XAI) techniques are essential to provide insights into how deep learning model make predictions. The project aims to develop a deep learning model for diagnosing IUGR using FHR signals and to apply explainability techniques to interpret the model's decisions. The focus is on using Local Interpretable Model-agnostic Explanations (LIME).

Existing explainability approaches generally fail to provide satisfactory explanations for high-dimensional multivariate time series data. In this project, LIME method has been readjusted for multivariate time-series data and applied to investigate our ResNet model predictions. Using LIME for time series data is crucial because it helps to explain complex patterns and relationships over time, making it easier to understand and trust the model's predictions. The results of Local Interpretable Model-agnostic Explanations (LIME) are interpreted in this document. Different parameter selections, such as the number of segments and the choice of perturbation function, are applied to see how these variations impact the LIME results. Additionally, the obtained LIME results have been interpreted in accordance with CTG physiology. This ensures that the explanations are not only accurate but also relevant to clinical practice, thereby improving the modelâ€™s practical utility and helping to bridge the gap between technical predictions and real-world applications.
